{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f585a10a",
   "metadata": {},
   "source": [
    "# Task 2 â€” Lucent Feature Visualization\n",
    "\n",
    "This notebook uses **lucent** to visualize what the CNN has learned on biased Colored-MNIST. It follows lucent_instructions.md strictly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7118717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. MODEL DEFINITION ---\n",
    "conv1_features = 8\n",
    "conv2_features = 16\n",
    "\n",
    "class ThreeLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThreeLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, conv1_features, kernel_size=5, padding=\"same\")\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2) \n",
    "        self.conv2 = nn.Conv2d(conv1_features, conv2_features, kernel_size=5, padding=\"same\")\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2) \n",
    "\n",
    "        self.fc1 = nn.Linear(conv2_features * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu_fc(self.fc1(x))\n",
    "        x = self.relu_fc(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8024b5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'model_weights.pth' not found. Visualizing untrained weights.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. SETUP & WEIGHT LOADING ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ThreeLayerCNN().to(device).eval()\n",
    "\n",
    "# Load weights (Replace 'model_weights.pth' with your actual filename)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('model_weights.pth', map_location=device))\n",
    "    print(\"Weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'model_weights.pth' not found. Visualizing untrained weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfa0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. OPTIMIZATION SETTINGS ---\n",
    "# MNIST is small (28x28), but Lucent works best with slightly larger dims for visualization clarity.\n",
    "# We will optimize a 32x32 image.\n",
    "img_size = 32 \n",
    "iterations = 512\n",
    "\n",
    "# Updated get_vis function\n",
    "def get_vis(obj, label):\n",
    "    # 1. Manually define the \"standard\" transforms Lucent uses\n",
    "    # For MNIST (28-32px), we use smaller jitter (2-4) so features stay centered\n",
    "    custom_transforms = [\n",
    "        transform.pad(4, mode='constant', constant_value=0.5),\n",
    "        transform.jitter(2),\n",
    "        transform.random_scale([0.9, 0.95, 1.05, 1.1]),\n",
    "        transform.random_rotate(list(range(-10, 11))),\n",
    "        transform.jitter(2),\n",
    "    ]\n",
    "\n",
    "    # 2. Use the parameterization (img_size matches your model's expected 28x28 or 32x32)\n",
    "    img_param = param.image(img_size)\n",
    "    \n",
    "    # 3. Call render with the manual transforms list\n",
    "    images = render.render_vis(\n",
    "        model, \n",
    "        obj, \n",
    "        img_param, \n",
    "        transforms=custom_transforms, # Use our manual list here\n",
    "        thresholds=(iterations,), \n",
    "        show_image=False, \n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    return images[-1], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff1620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. DEFINING OBJECTIVES ---\n",
    "# Row 1: Early Conv (Looking for color/noise filters)\n",
    "early_targets = [f\"conv1:{i}\" for i in range(4)]\n",
    "\n",
    "# Row 2: Late Conv (Looking for complex shortcut patterns)\n",
    "late_targets = [f\"conv2:{i}\" for i in range(4)]\n",
    "\n",
    "# Row 3: Class Neurons (The 'Prototypes' for specific digits)\n",
    "# We pick digits often confused or heavily biased (0, 3, 7, 9)\n",
    "class_targets = [0, 3, 7, 9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563823ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing images... this may take a moment.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Optimize Early Channels\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m early_targets:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     img, lbl = \u001b[43mget_vis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjectives\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconv1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEarly: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     results.append((img, lbl))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Optimize Late Channels\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mget_vis\u001b[39m\u001b[34m(obj, label)\u001b[39m\n\u001b[32m     20\u001b[39m img_param = param.image(img_size)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 3. Call render with the manual transforms list\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m images = \u001b[43mrender\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender_vis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use our manual list here\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_image\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m images[-\u001b[32m1\u001b[39m], label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/precog_application/cv-task/venv/lib/python3.13/site-packages/lucent/optvis/render.py:50\u001b[39m, in \u001b[36mrender_vis\u001b[39m\u001b[34m(model, objective_f, param_f, optimizer, transforms, thresholds, verbose, preprocess, progress, show_image, save_image, image_name, show_inline, fixed_image_size)\u001b[39m\n\u001b[32m     46\u001b[39m     param_f = \u001b[38;5;28;01mlambda\u001b[39;00m: param.image(\u001b[32m128\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# param_f is a function that should return two things\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# params - parameters to update, which we pass to the optimizer\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# image_f - a function that returns an image as a tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m params, image_f = \u001b[43mparam_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     53\u001b[39m     optimizer = \u001b[38;5;28;01mlambda\u001b[39;00m params: torch.optim.Adam(params, lr=\u001b[32m5e-2\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# --- 5. EXECUTION ---\n",
    "print(\"Optimizing images... this may take a moment.\")\n",
    "results = []\n",
    "\n",
    "# Optimize Early Channels\n",
    "for t in early_targets:\n",
    "    img, lbl = get_vis(objectives.channel(\"conv1\", int(t.split(':')[-1])), f\"Early: {t}\")\n",
    "    results.append((img, lbl))\n",
    "\n",
    "# Optimize Late Channels\n",
    "for t in late_targets:\n",
    "    img, lbl = get_vis(objectives.channel(\"conv2\", int(t.split(':')[-1])), f\"Late: {t}\")\n",
    "    results.append((img, lbl))\n",
    "\n",
    "# Optimize Class Neurons\n",
    "for c in class_targets:\n",
    "    # 'fc3' is the final layer; we optimize the neuron index 'c'\n",
    "    img, lbl = get_vis(objectives.neuron(\"fc3\", c), f\"Class: {c}\")\n",
    "    results.append((img, lbl))\n",
    "\n",
    "# --- 6. PLOTTING THE COMPOSITE GRID ---\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, (img, lbl) in enumerate(results):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(lbl, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Feature Visualization: Probing Shortcut Learning\", fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
